The `odespy` package makes it easy to specify an ODE problem in
Python and get it solved by a wide variety of different numerical
methods.

======= Motivation =======

The `odespy` package grew out of the desire to have a unified interface
to lots of different methods and software for ODEs. Consider the
ODE problem
!bt
\[ y'' = 3 (1 - u^2) y' + y,\quad y(0)=2,\ y'(0)=1,
!et
known as the van der Pool oscillator. The solution is
desired at 150 equally spaced time levels in
the interval `[0, 30]`.
We want to solve this problem
by three well-known software routines:

  o `LSODE` from ODEPACK (adaptive Adams and BDF methods)
  o `ode45` from MATLAB (adaptive Runge-Kutta 4-5-th order)
  o `vode` from Python (adaptive Adams and BDF methods)

All of these routines require the ODE problem to be on the form
$u'=f(u,t)$, which means that the
the second-order differential equation must be
recast as a system of two ODEs, and we have to identify the two components
of the $f(u,t)$ function:
!bt
\[ u^{(0)}'(t)= u^{(1)},\quad u^{(1)'(t)=3(1-(u^{(0)}})^2u^{(1)} + u^{(0)}\]
\[ u^{(0)}(0)=2,\quad u^{(1)}(0)=1.
\[ f^{(0)}(u^{(0)}, u^{(1)}, t)= u^{(1)},\quad f^{(1)(u^{(0)}, u^{(1)}, t)=3(1-(u^{(0)}})^2u^{(1)} + u^{(0)}\]
!et
The mentioned ODE software needs a specification of the $f(u,t)$
through some user-written function that takes $u$ and $t$ as input and
delivers $f$ as output.

__LSODE.__ Using `LSODE` and other ODEPACK routines requires the
ODE problem to be specified in FORTRAN and the solver to be called
from FORTRAN:
!bc fpro
     PROGRAM MAIN
     EXTERNAL F
     INTEGER I, IOPT, IOUT, ISTATE, ITASK, ITOL, IWORK,
    1   LRW, LIW, MF, NEQ, NOUT
     DOUBLE PRECISION ATOL, T, TOUT, RTOL, RWORK, U, URR
     DIMENSION U(2), RWORK(52), IWORK(20), U1(5), U2(5)
     NEQ = 2
C    SET ADAMS METHOD:
     MF = 10
C    LET TOLERANCES BE SCALARS (NOT ARRAYS):
     ITOL = 1
C    USE ONLY ABSOLUTE TOLERANCE:
     RTOL = 0.0D0
     ATOL = 1.0D-6
     LRW = 52
     LIW = 20
C    NUMBER OF TIME STEPS:
     NOUT = 150
     T = 0.0D0
C    FINAL TIME:
     TOUT = 30.0D0
C    INITIAL CONDITIONS
     U(1)= 2.0D0
     U(2) = 0.0D0
     ITASK = 1
     ISTATE = 1
C    CALL ADAPTIVE TIME STEPPING AT EACH OF THE TARGET TIME LEVELS
     DO 100 IOUT = 1, NOUT
       CALL DLSODE(F,NEQ,U,T,TOUT,ITOL,RTOL,ATOL,ITASK,
    1     ISTATE,IOPT,RWORK,LRW,IWORK,LIW,JAC,MF)
       U1(IOUT) = U(1)
       U2(IOUT) = U(2)
       TOUT = TOUT + 2.0D-1
 100 CONTINUE
     END

     SUBROUTINE F(NEQ, T, U, UDOT)
     INTEGER NEQ
     DOUBLE PRECISION T, U, UDOT
     DIMENSION U(2), UDOT(2)
     UDOT(1) = U(2)
     UDOT(2) = 3.0D0*(1.0D0 - U(1)*U(1))*U(2) - U(1)
     RETURN
     END
!ec

__MATLAB.__
The problem can be solved with very compact code in MATLAB. The definition
of the ODE system, the $f(u,t)$ function, must be placed in
a function in a file, say `myode.m`:
!bc mcod
function F = myode(t, u);
F(1,1) = u(2)
F(2,1) = 3*(1 - u(1)*u(1))*u(2) - u(1)
!ec
In MATLAB we can then solve the problem by
!bc ipy
>> options = odeset('RelTol',0.0,'AbsTol',1e-6);
>> tspan = [0 30];
>> u0 = [2; 0]
>> [t, u] = ode45('myode', tspan, u0, options]);
!ec

__Python.__
Calling up the `vode` method from the `scipy` library in Python
also results in compact code:
!bc pycod
from scipy.integrate import ode

u0 = [2.0, 0.0]

def f(t, u):
    return [u[1], 3.*(1. - u[0]*u[0])*u[1] - u[0]]

# integration
r = ode(f).set_integrator('vode', method='adams',
                          order=10, rol=0, atol=1e-6,
                          with_jacobian=False)
r.set_initial_value(u0, 0)
T = 30
dt = T/150.
u = [];  t = []
while r.successful() and r.t <= T:
    r.integrate(r.t + dt)
    u.append(r.y);  t.append(r.t)
!ec
Suppose you want to compare these methods and their implementations.
This requires three different main programs and (worst) three
different implementations of the definition of the mathematical problem.
Some has the signature $f(u,t)$ while others require $f(t,u)$, and
this difference between packages is often a cause of errors.

__Unified Interface.__ The `odespy` package provides a unified
interface to all the three mentioned types of methods so we can do
this:

@@@CODE src-odespy/motivation1.py

The same `f` and the same call syntax can be reused across methods
and software.




======= Basic Usage =======

===== Overview =====

The typical usage of the tools consists of six steps. These are
outlined in generic form below.

=== Step 1 ===

Write the ODE problem on the generic form $u' = f(u, t)$,
where $u(t)$ is the unknown function to be solved for, or a vector
of unknown functions of time in case of a system of ODEs.

=== Step 2 ===

Implement the right-hand side function $f(u, t)$, which
defines the ODEs to be solved, as a Python function `f(u, t)`.  The
argument `u` is either a `float` object (in case of a scalar ODE) or a
`numpy` array object (in case of a system of ODEs).  Alternatively, a
class with a `__call__` method can be used instead of a plain
function.  Some tools in this package also allow implementation of $f$
in Fortran or C for increased efficiency.

=== Step 3 ===

Create a method object
!bc pycod
method = classname(f)
!ec
where `classname` is the name of a class in this package implementing
the desired numerical method.

Many solver classes has a range of parameters that the user can set to
control various parts of the solution process. The parameters are
documented in the doc string of the class (`pydoc classname` will list
the documentation in a terminal window). One can either specify parameters
at construction time, via extra keyword arguments to the constructor,
!bc pycod
method = classname(f, prm1=value1, prm2=value2, ...)
!ec
or at any time using the `set` method:
!bc pycod
method.set(prm1=value1, prm2=value2, prm3=value3)
...
method.set(prm4=value4)
!ec

=== Step 4 ===

Set the initial condition, $u(0)=U_0$,
!bc pycod
method.set_initial_condition(U0)
!ec
where `U0` is either a number, for a scalar ODE, or a sequence (list, tuple,
`numpy` array), for a system of ODEs.

=== Step 5 ===

Solve the ODE problem, which means to compute $u(t)$ at
some discrete user-specified time points $t_i$, for
$i=0,1,\ldots,n$|$i=0,1,...,n$.
!bc pycod
T = ...  # end time
time_points = numpy.linspace(0, T, n+1)
u, t = method.solve(time_points)
!ec
In case of a scalar ODE, the returned solution `u` is a one-dimensional
`numpy` array where `u[i]` holds the solution at time point `t[i]`.
For a system of ODEs, the returned `u` is a two-dimensional `numpy`
array where `u[i,j]` holds the solution of the $j$-th unknown
function at the $i$-th time point `t[i]` ($u_j(t_i)$ in mathematics
notation).

The `time_points` array specifies the time points where we want the
solution to be computed. The returned array `t` is the same as
`time_points`.  The simplest numerical methods in the `odespy`
package apply the `time_points` array directly in the solution. That
is, the time steps used are given by `time_points[i] -
time_points[i-1]`, for `i=0,1,...,len(time_points)-1`.  Some more
advanced adaptive methods compute the time steps internally. Then the
`time_points` array is just a specification of the time points where
we want to know the solution.

[hpl: Need to document how the tp array is used.]

[hpl: Need to comment on storage.]

The `solve` method in solver classes also allows a second argument,
`terminate`, which is a user-implemented Python function specifying
when the solution process is to be terminated. For example,
terminating when the solution reaches an asymptotic (known) value
`a` can be done by
!bc pycod
def terminate(u, t, step_no):
    # u and t are arrays. Most recent solution is u[step_no].
    tolerance = 1E-6
    return True if abs(u[step_no] - a) < tolerance else False

u, t = method.solve(time_points, terminate)
!ec
The arguments transferred to the `terminate` function are the
solution array `u`, the corresponding time points `t`, and
an integer `step_no` reflecting the most recently computed `u`
value. That is, `u[step_no]` is most recently computed value of $u$.
(The array data `u[step_no+1:]` will typically be zero as these
are uncomputed future values.)


=== Step 6 ===

Extract solution components for plotting and further analysis.
Since the `u` array returned from `method.solve` stores all unknown
functions at all discrete time levels, one usually wants to extract
individual unknowns as one-dimensional arrays. Here is an example
where unknown number $0$ and $k$ are extracted in individual arrays
and plotted:
!bc pycod
u_0 = u[:,0]
u_k = u[:,k]

from matplotlib.pyplot import plot, show
plot(t, u_0, t, u_k)
show()
!ec

===== Basic Example: Logistic Growth =====
label{ode:sec:exgr}

Our first example concerns the simple scalar ODE problem $u'=au(1-u/R)$,
$u(0)=A$, where $A>0$, $a>0$, and $R>0$ are known constants. This is
a common model for population dynamics in ecology where $u$ is the
number of individuals, $a$ the initial growth rate, $R$ is the
maximum number of individuals that the environment allows (the so-called
*carrying capacity* of the environment).
Using a standard
Runge-Kutta method of order four, the code for solving the problem in
the time interval $[0,10]$, with $N=30$ time steps, looks like this:

@@@CODE src-odespy/logistic1.py  fromto: def f@def u_exact

With the `RK4` method and other non-adaptive methods
the time steps are dictated by the `time_points` array.
A constant time step of size is implied in the present example.

We can easily plot the numerical solution and compare with the exact
solution (which is known for this equation):

@@@CODE src-odespy/logistic1.py  fromto: def u_exact@

FIGURE: [figs-odespy/logistic1.png, width=400] Solution of the logistic equation with the 4-th order Runge-Kutta method (solid line) and comparison with the exact solution (dots).

===== Parameters in the Right-Hand Side Function =====
label{ode:sec:exgr:farg}

The right-hand side function and all physical parameters are often
lumped together in a class, for instance,

@@@CODE src-odespy/logistic.py  fromto: class Logistic@import odespy

Note that introducing local variables like `a` and `R`, instead of
using `self.a` and `self.A`, makes the code closer to mathematics.
This can be convenient when proof reading the implementation of
complicated ODEs.

The numerical solution is found by

@@@CODE src-odespy/growth2.py  fromto: import odespy@from matplotlib

Instead of having the problem parameters `a` and `R` in the ODE as
global variables or in a class, we may include them as extra arguments
to `f`, either as positional arguments or as keyword
arguments. Positional arguments can be sent to `f` via the constructor
argument `f_args` (a list/tuple of variables), while a dictionary
`f_kwargs` is used to transfer keyword arguments to `f` via the
constructor:

@@@CODE src-odespy/logistic3.py  fromto: def f@solver\.set_initial

Here we used keyword arguments in `f`, but in general a mix
of positional and keword arguments can be used:
!bc pycod
def f(u, t, arg1, arg2, arg3, ..., kwarg1=val1, kwarg2=val2, ...):
    ...

method = odespy.classname(f,
                    f_args=[arg1, arg2, arg3, ...],
                    f_kwargs=dict(kwarg1=val1, kwarg2=val2, ...))

# Alternative setting of f_args and f_kwargs
solver.set(f_args=[arg1, arg2, arg3, ...],
           f_kwargs=dict(kwarg1=val1, kwarg2=val2, ...))
!ec
Solvers will call `f` as `f(u, t, *f_args, **f_kwargs)`.

===== Continuing a Previous Simulation =====

It is easy to simulate for some time interval $[0, T_1]$,
the continue with $u(T_1)$ as new initial condition and simulate for
$t$ in $[T_1, T_2]$ and so on. Let us divide the time
domain into subdomains and compute the solution for
each subdomain in sequence. The following program performs the steps.

@@@CODE src-odespy/logistic4.py  fromto: def f@show\(

===== Termination Criterion for the Simulation =====

We know that the solution $u$ of the logistic equation approaches
$R$ as $t\rightarrow\infty$|$t becomes large$. Instead of
using a trial and error process for determining an appropriate
time integral for integration, the `solver.solve` method accepts
a user-defined function `terminate` that can be used to implement
a criterion for terminating the solution.
Mathematically, the relevant criterion is
$||u-R||<\hbox{tol}$|$||u - R|| < tol$, where tol is an acceptable
tolerance, say $100$ in the present case where $R=10^5$.
The `terminate` function implements the criterion
and returns true if the criterion is met:

@@@CODE src-odespy/logistic5.py  fromto: def terminate@u, t =

Note that the simulation is anyway stopped for $t > T$ so $T$
must be large enough for the termination criterion to be reached.
With a `terminate` function it is also convenient to specify the
time step `dt` and not the total number of time steps.

A complete program can be as follows:

@@@CODE src-odespy/logistic5.py

===== An Class-Based Implementation =====

The previous code example can be recast into a more class-based
("object-oriented programming") example. We lump all data related
to the problem (the "physics") into a problem class `Logistic`, while
all data related to the numerical solution and its quality are
taken care of by class `Solver`. The code below illustrates
the idea:

@@@CODE src-odespy/logistic6.py

===== Using Other Symbols =====

The `odespy` package applies `u` for the unknown function or
vector of unknown functions and `t` as the name of the independent
variable. Many problems involve other symbols for functions and
independent variables. These symbols should be reflected in the code.
For example, here is a coding example involving the logistic
equation written as $y'(x)=au(x)(1-u(x)/R(x))$, where now $R=R(x)$
is considered. Following the setup froom the very first program
above solving the logistic ODE, we can easily introduce our own
nomenclature:

@@@CODE src-odespy/logistic7.py

As shown, we use `y` for `u`, `x` for `t`, and `x_points` instead
of `time_points`.

===== Example: The Pendulum Equation =====

The angle $\theta$|$theta$ of a pendulum with mass $m$ and length $L$
is governed by the equation
(neglecting air resistance for simplicity)
# #if FORMAT == "latex" or FORMAT == "sphinx"
!bt
\[
mL\ddot\theta + mg\sin\theta = 0,\quad \theta (0)=\Theta,\
\dot\theta (0)=0 .
\]
!et
A dot over $\theta$ implies differentiation with respect to time.
The ODE can be written as $\ddot\theta + c\sin\theta=0$ by
introducing $c = g/L$.
# #else
# pure ASCII
_theta'' + c*sin(theta) = 0_, with _theta(0) = Theta_ and _theta'(0) = 0_.
# #endif
This problem must be expressed as a first-order ODE system if it is
going to be solved by the tools in the `odespy` package.
# #if FORMAT == "latex" or FORMAT == "sphinx"
Introducing $\omega = \dot\theta$ (the angular velocity) as auxiliary
unknown, we get the system
!bt
\begin{align*}
\dot\theta &= \omega,\\
\dot\omega &= -c\sin\theta,
\end{align*}
!et
with $\theta(0)=\Theta$ and $\omega(0)=0$.
# #else
Introducing omega as theta', we get the first equation as
_theta' = omega_ and the second as _omega' = -c*sin(theta)_.
The initial conditions are _theta(0) = Theta_ and _omega'(0) = 0_.
# #endif

Now the `f` function must return a list or array with the two
right-hand side functions:

@@@CODE src-odespy/osc1a.py  fromto: def f@method =

Note that when we have a system of ODEs with `n` components, the `u`
object sent to the `f` function is an array of length `n`,
representing the value of all components in the ODE system at time `t`.
Here we extract the two components of `u` in separate local variables
with names equal to what is used in the mathematical description of
the current problem.

The initial conditions must be specified as a list:

@@@CODE src-odespy/osc1a.py  fromto: method =@freq =

To specify the time points we here first decide on a number of periods
(oscillations back and forth) to simulate and then on the time resolution
of each period. (When $\Theta$|$Theta$ is small we can replace
$\sin\theta$|$sin(theta)$ by $\theta$|$theta$ and find an analytical
solution
$\theta (t)=\Theta\cos\left(\sqrt{c}t\right)$|$theta(t) = Theta*cos(sqrt(c)*t)$
whose period is $2\pi/\sqrt{c}$|$2*pi/sqrt(c)$.)

@@@CODE src-odespy/osc1a.py  fromto: freq =@theta =

The `u` returned from `method.solve` is a two-dimensional array, where the
columns hold the various solution functions of the ODE system. That is,
the first column holds $\theta$|$theta$ and the second column holds
$\omega$|$omega$. For convenience we extract the individual solution
components in individual arrays:

@@@CODE src-odespy/osc1a.py  fromto: theta =@import matplotlib

Looking at the plot reveals that the numerical solution has
an alarming feature: the amplitude grows (indicating increasing
energy in the system). Changing `T` to 28 periods instead of 10
makes the numerical solution explode.

FIGURE: [figs-odespy/exos1a.png, width=500] Comparison of large-amplitude numerical solution with corresponding analytical solution (derived for small amplitudes).

Changing solution method is a matter of substituting `Heun` by `RK4`,
for instance:

@@@CODE src-odespy/osc1b.py  fromto: method =@method\.set_initial

The amplitude is now correct, but the numerical solution has a different
frequency than the inaccurate analytical "solution".

FIGURE: [figs-odespy/exos1b.png, width=500] Switching to a 4-th order Runge-Kutta method improves the numerical solution.

Changing $\Theta$|$Theta$ to a small value, say 0.05, makes the two curves
coincide. The next section shows how easy it is to run a problem with a set
of numerical methods.


===== Example: Testing Several Methods =====

We shall now make a more advanced solver by
extending the previous example. More specifically, we shall

  * represent the right-hand side function as class,
  * compare several different solvers,
  * compute error of numerical solutions.

Since we want to compare numerical errors in the various
solvers we need a test problem where the exact solution is known.
Approximating $\sin(\theta)$|$sin(theta)$ by $\theta$|$theta$
(valid for small $\theta$|$theta$), gives the ODE system
# #if FORMAT == "latex" or FORMAT == "sphinx"
!bt
\begin{align*}
\dot\theta &= \omega,\\
\dot\omega &= -c\theta,
\end{align*}
!et
with $\theta(0)=\Theta$ and $\omega(0)=0$.
# #else
_theta' = omega_ and _omega' = -c*theta_ with
initial conditions _theta(0) = Theta_ and _omega'(0) = 0_.
# #endif

Right-hand side functions with parameters can be handled by
including extra arguments via the `f_args` and `f_kwargs` functionality,
or by using a class where the parameters are attributes and
a `__call__` method makes the class instance callable as a function.
Section ref{ode:sec:exgr:farg} exemplifies the details.
A minimal class representation of the right-hand side
function in the present case looks like this:
!bc
class Problem:
    def __init__(self, c, Theta):
        self.c, self.Theta = float(c), float(Theta)

    def __call__(self, u, t):
        theta, omega = u;  c = self.c
        return [omega, -c*theta]

problem = Problem(c=1, Theta=pi/4)
!ec
It would be convenient to add an attribute `period` which holds
an estimate of the period of oscillations as we need this for
deciding on the complete time interval for solving the differential
equations. An appropriate extension of class `Problem` is therefore

@@@CODE src-odespy/osc2.py  fromto: class Problem@import odespy

The second extension is to loop over many solvers. All
solvers can be listed by
!bc ipy
>>> import odespy, pprint
>>> solvers = list_all_solvers()
>>> for solver in solvers:
...   print solver
...
AdamsBashMoulton2
AdamsBashMoulton3
AdamsBashforth2
AdamsBashforth3
AdamsBashforth4
AdaptiveResidual
Backward2Step
BackwardEuler
BogackiShampine
CashKarp
Dop853
Dopri5
DormandPrince
Fehlberg
Euler
ForwardEuler
Heun
Leapfrog
LeapfrogFiltered
Lsoda
Lsodar
Lsode
Lsodes
Lsodi
Lsodis
Lsoibt
MidpointImplicit
MidpointIter
MyRungeKutta
MySolver
RKC
RKF45
RK2
RungeKutta2
RK3
RungeKutta3
RK4
RungeKutta4
RKFehlberg
SymPy_odefun
ThetaRule
Trapezoidal
Vode
!ec
A similar function, `list_available_solvers`, returns a list of the
names of the solvers that are available in the current installation
(e.g., the `Vode` solver is only available if the comprehensive
`scipy` package is installed).
This is the list that is usually most relevant.

For now we explicitly choose a subset of the commonly available solvers:

@@@CODE src-odespy/osc2.py  fromto: import odespy@N_per_period =

It will be evident that the `ThetaRule` solver with `theta=0` and
`theta=1` (Forward and Backward Euler methods) gives growing and
decaying amplitudes, respectively, while the other solvers are
capable of reproducing the constant amplitude of the oscillations of
in the current mathematical model.

The loop over the chosen solvers may look like

@@@CODE src-odespy/osc2.py  fromto: N_per_period =@

FIGURE: [figs-odespy/exos2.png, width=500] Comparison of solutions.

We can extend this program to compute the error in each numerical
solution for different time step sizes.
Let `results` be a dictionary with the method name as
key, containing two sub dictionaries `dt` and `error`, which hold
a sequence of time steps and a sequence of corresponding
errors, respectively. The errors are computed by subtracting
the numerical solution from the exact solution,
!bc pycod
theta_exact = lambda t: problem.Theta*numpy.cos(sqrt(problem.c)*t)
u, t = method.solve(time_points)
theta = u[:,0]
error = numpy.abs(theta_exact(t) - theta)
!ec
The so-called L2 norm of the `error` array is a suitable
scalar error measure (square root of total error squared and integrated,
here numerically):
!bc pycod
error_L2 = sqrt(numpy.sum(error**2)/dt)
!ec
where `dt` is the time step size.

Typical loops over solvers and resolutions look as follows:

@@@CODE src-odespy/osc3.py  fromto: T = num_periods@\# Print

Assuming the error to be of the form $C\Delta t^r$, we can estimate
$C$ and $r$ from two consequtive experiments to obtain a sequence
of $r$ values which (hopefully) convergences to a value that we can
view as the empirical convergence rate of a method.
Given the sequence of time steps and errors, a function in
the `scitools` package automatically computes the sequence of
$r$ values:

@@@CODE src-odespy/osc3.py  fromto: \# Analyze@

With 4 periods we get
!bc dat
ThetaRule(theta=0)   r: 2.9, 1.9, 1.4, 1.2 E_min=1.1E-01
RK2                  r: 2.1, 2.0, 2.0, 2.0 E_min=8.4E-04
ThetaRule(theta=1)   r: 0.3, 0.5, 0.7, 0.8 E_min=9.0E-02
RK4                  r: 4.0, 4.0, 4.0, 4.0 E_min=2.6E-08
ThetaRule            r: 2.0, 2.0, 2.0, 2.0 E_min=4.2E-04
Leapfrog             r: 2.1, 2.0, 2.0, 2.0 E_min=8.5E-04
LeapfrogFiltered     r: 0.2, 0.4, 0.6, 0.8 E_min=1.3E-01
!ec
The rates of the Forward and Backward Euler methods (1st and 3rd line) have
not yet converged to unity, as expected, while the 2nd-order
Runge-Kutta method, Leapfrog, and the $\theta$|$theta$-rule with $\theta =0.5$
(`ThetaRule` with default value of `theta`) shows the expected
$r=2$ value. The 4th-order Runge-Kutta holds the promise of being of 4th
order, while the filtered Leapfrog method has slow convergence and
a fairly large error, which is also evident in the previous figure.

Extending the time domain to 20 periods makes many of the
simplest methods inaccurate and the rates computed on coarse
time meshes are irrelevnat:
!bc dat
ThetaRule(theta=0)   r: 10.4, 22.0, 18.2, 10.4 E_min=3.3E+02
RK2                  r: 51.4, 16.1, 2.3, 2.1 E_min=1.1E-01
ThetaRule(theta=1)   r: 11.2, 0.0, 0.1 E_min=5.0E-01
RK4                  r: 1.0, 3.6, 4.0, 4.0 E_min=8.2E-05
ThetaRule            r: 87.8, 0.2, 1.7, 2.0 E_min=5.3E-02
Leapfrog             r: 95.9, 18.0, 1.0, 2.0 E_min=1.1E-01
LeapfrogFiltered     r: -0.0, 121.2, 0.3, 0.1 E_min=5.2E-01
!ec

===== Example: Solving a Stochastic Differential Equation =====

We consider an oscillator driven by stochastic white noise:
!bt
\[ x''(t) + bx'(t) + cx(t) = N(t),\ x(0)=X,\ x'(0) =0,\]
!et
where $N(t)$ is the white noise computed discretely as
!bt
\[ N(t_i) \approx \sigma\frac{\Delta W_i}{\sqrt{t_{i+1}-t_i}},\]
!et
where $\Delta W_1,\Delta W_2,\ldots$ are independent normally
distributed random variables with mean zero and unit standard
deviation, and $\sigma$ is the strength of the noise.
The idea is that $N(t)$ provides an excitation containing "all" frequencies,
but the oscillator is a strong filter: with low damping one of the
frequencies in $N(t)$ will hit the resonance frequency
$\sqrt{c}/(2\pi)$ which will
then dominate the output signal $x(t)$.

The noise is additive in this stochastic differential equation so
there is no difference between the Ito and Stratonovich interpretations
of the equation.

The challenge with this model problem is that stochastic differential
equations do not fit with the user interface offered by `odespy`,
since the right-hand side function is assumed to be dependent only
on the solution and the present time (`f(u,t)`), and additional
user-defined parameters, but for the present problem the right-hand
side function needs information about $N(t)$ and hence
the size of the current time step. We can solve this issue by
having a reference to the solver in the right-hand side function,
precomputing $N(t_i)$ for all time intervals $i$, and using the `n`
attribute in the solver for selecting the right force term (recall
that some methods will call the right-hand side function many times
during a time interval - all these calls must use the same value of
the white noise).

The right-hand side function must do many things so a class is
appropriate:

@@@CODE src-odespy/exsos1.py class WhiteNoise@from numpy

We can easily compare different methods:

@@@CODE src-odespy/exsos1.py f = White@savefig

FIGURE: [figs-odespy/exsos1.png, width=500] Oscillator driven by white noise.

The `Heun` and `RK2` methods give coinciding solutions while
the `ForwardEuler` method gives too large amplitudes.
The frequency is 0.5 (period 2) as expected.

In this example the white noise force is computed only once since
the `f` instance is reused in all methods. If a new `f` is created
for each method, it is crucial that the same seed of the random
generator is used for all methods, so that the time evolution of
the force is always the same - otherwise the solutions will be
different.


# #ifdef EXTRA
===== Example: Solving a Partial Differential Equation =====

# #endif



======= Inner Workings of the Package =======

There are three basic entities when solving ODEs numerically: the
definition of the *ODE system*, the *time-stepping method*, and
the *solver* that runs the "time loop" and stores results.

The information about the ODE system is made very simple: the user
provides 1) an object that can be called as a Python function `f(u,
t)`, and 2) an array or list with the initial values.  Some users will
wrap this information in their own data structures, e.g., a class.

The time-stepping method and the algorthm for calling the time-stepping
are collected in a solver class. All the solver classes are
related in a class hierarchy. Each solver class initialized by the right-hand
side function (`f`) and an optional set of parameters for controlling
various parts of the solution process. The solver object is also
used to set the initial condition (`set_initial_condition`) and
to run the solution process (`solve`). The time-stepping scheme
is isolated in a method `advance` in the solver classes, but
for some schemes or external software packages the separation of
doing one step and doing the whole time integration is less feasible.

The package does not interact with visualization tools - the array
containing the solution is returned to the user and must be further
processed and visualized in the user's code.

Below we describe how the classes in the solver hierarchy work and how
parameters are registered and initialized.

===== Solver Parameters =====
label{odes:parameters}

The `solver` module defines a global dictionary `_parameters` holding
all legal parameters. Other modules imports this `_parameters` dict
and updates it with their own additional parameters.

For each parameter the `_parameters` dict stores the parameter name, a
default value, a description, the legal object type for the value of
the parameter, and other quantities if needed. A typical example
is
!bc
_parameters = dict(
...

f = dict(
    help='Right-hand side f(u,t) defining the ODE',
    type=callable),

f_kwargs = dict(
    help='Extra keyword arguments to f: f(u, t, *f_args, **f_kwargs)',
    type=dict,
    default={}),

theta = dict(
    help="""Weight in [0,1] used for
"theta-rule" finite difference approx.""",
    default=0.5,
    type=(int,float),
    range=[0, 1])

...
}
!ec

Each solver class defines a (static) class variable
`_required_parameters` for holding the names of all required
parameters (in a list). In addition, each solver class defines another
class variable `_optional_parameters` holding the names of all the
optional parameters. The doc strings of the solver classes are
automatically equipped with tables of required and optional
parameters.

The optional parameters of a class consists of the optional parameters
of the superclass and those specific to the class. The typical
initialization of `_optional_parameters` goes like this:
!bc pycod
class SomeMethod(ParentMethod):
    _optional_parameters = ParentMethod._optional_parameters + \
                           ['prm1', 'prm2', ...]
!ec
where `prm1`, `prm2`, etc. are names registered in the global
`_parameters` dictionary.

From a user's point of view, the parameters are set either at
construction time or through the `set` function:
!bc py
>>> from odespy import RK2
>>> def f(u, t, a, b=0):
...   return a*u + b
...
>>> method = RK2(f, f_kwargs=dict(b=1))
>>> method.f_kwargs
{'b': 1}
>>> method.set(f_args=(3,))
>>> method.f_args
(3,)
>>> # Get all registered parameters in the method instance
>>> method.get()
{'f_kwargs': {'b': 1}, 'f_args': (3,), 'complex_valued': False,
'name of f': 'f'}
!ec
The `set` method sets parameters through keyword arguments and can
take an arbitrary collection of such arguments:
!bc cod
method.set(name1=value1, name2=value2, name3=value3, ...)
!ec

The `get` method returns the parameters and their values as a dictionary.
(Note that the `'f'` key, which one might expect to appear in the
returned dictionary of parameters, are omitted because it is always
a lambda function wrapping the user's `f` function such that the
returned value is guaranteed to be a `numpy` array. Instead,
there is an entry `'name of f'` which reflects the name of the
user-supplied function.)


===== Solver Classes =====

Each solver in this package is implemented as a class in a class hierarchy.
Basic, common functionality is inherited from super classes, and the
actual solver class implements what is specific for the method in question.


=== The Super Class ===

Class `Solver` is the super class of the hierarchy. Its constructor
requires one mandatory argument: the right-hand side of the ODE,
$f(u,t)$, coded as a Python function `f(u, t)` or given as a string
containing code in a compiled language (Fortran, for instance)
implementing the right-hand side.  Additional keyword arguments can be
provided to set parameters of the solver.

The constructor performs a set of tasks that are common to all
the subclass solvers:

 o The set of optional and required parameters of a particular solver
   is loaded into `self._parameters` such that this dictionary
   can be used to look up all parameters of the solver.

 o Representation of $f(u, t)$ (or the Jacobian) in a compiled language
   is compiled into an extension module.

 o The solver-specific method `adjust_parameters` is called to allow
   the programmer of a solver to manipulate `self._parameters`.
   For example, some existing or new parameters may be modified or set
   according to the value of other parameters.

 o All key-value pairs in `self._parameters` are mirrored by class
   attributes. The computations and the `set` and `get` methods will
   make use of the attributes rather than the `self._parameters` dict
   to extract data.  For example, the value of
   `self._parameters['myvar']` becomes available as `self.myvar` and
   in the algorithms we use `self.myvar`, perhaps with a test
   `hasattr(self, 'myvar')` test or a `try`-`except` clause (catching
   an `AttributeError`).

 o The `set` method is called with all keyword arguments to the
   constructor, which then modifies the default values of the
   parameters.

 o The `f` function is wrapped in a lambda function such that
   `f(u, t)` is guaranteed to return an array (in case the user
   returns a list or scalar for convenience).

 o The `initialize` method is called to finalize the tasks in
   the constructor. The most common use of this method in subclasses
   is to import extension modules that the solver depends on and
   provide an error message if the extension modules are not available.
   If they are, the modules are normally stored through an attribute
   of the subclass.

 o The `switch_to` method returns a new solver initialized with all
   parameters of the current solver that are legal in the new solver.
   The method is useful when trying out a range of solvers for a
   problem.

Let `method` some instance of a subclass in the hierarchy. The
following calls are possible (through inheriting common convenience
methods in the super class `Solver`):

 * `repr(method)`: return the subclass name along with all
   registered parameters and their values. This string provides
   complete information on the state of a subclass.

 * `str(method)`: return a short pretty print string reflecting
   the name of the method and the value of parameters that
   must be known to uniquely define the numerical method.
   This string is what one would use as legends in a plot or
   as method identifier in a table.

 * `method.get_parameter_info`: return or print all registered
   parameters for the current solver and all properties for
   each parameter.

After the constructor is called, `method.set_initial_condition` is
called to set the initial condition, and then `solve` is called.
The `solve` method features the following steps:

 o Convert `time_points` to a `numpy` array.
 o Call `initialize_for_solve` (implemented in subclasses) to
   precompute whatever is needed before the time loop.
   The super class allocates storage for the solution and
   loads the initial condition into that data structure.
   Any subclass implementation of `initialize_for_solve` must therefore
   also call this method in its super class.
 o Call `validate_data` to check if the data structures are consistent
   before starting the computations. Subclass implementations of
   this method must call the super class' version of the method.
 o Run a loop over all time levels `n` and call `advance` (implemented
   in subclasses) at each level to advance the solution from
   time `t[n]` to `t[n+1]`. Also call `terminate` so that the
   user code can analyze and work with the solution.

Some subclasses will override the `solve` method and provide their own,
but most subclasses just inherits the general one and implement
the `advance` method.

All classes have a set of attributes:

 o `users_f` holding the user's function for $f(u, t)$
   (implicit solvers will have a corresponding `users_jac` for
   the user's Jacobian),
 o one attribute for each parameter in the class,
 o `u`: 1D `numpy` array holding the solution for a scalar ODE and
   a 2D array in case of a system of ODEs. The first index
   denotes the time level.
 o `t`: the time levels corresponding to the first index in the `u` array.
 o `quick_description`: a short one-line description of the method (this
   variable is static in the class, i.e., declared outside any method).

Most classes will also define two additional static variables,
`_required_parameters` and `_optional_parameters` as explained
in Section ref{odes:parameters}.

=== Other Superclasses ===

There are superclasses `SolverImplicit` for implicit methods,
`Adaptive` for adaptive methods, 'RungeKutta' for general
Runge-Kutta methods, `Ode_scipy` for interfaces to ODE solvers
in `scipy`, and `Odepack` for interfaces to the ODEPACK family
of solvers.


=== A Very Simple Subclass ===

To implement a simple explicit scheme for solving a scalar ODE or a system
of ODEs, you only need to write a subclass of `Solver` with an
`advance` method containing the formula that updates the solution from
one time level to the next. For example, the Forward Euler scheme
reads
!bt
\[ u_{n+1} = u_n + \Delta t f(u_n, t_n),\]
!et
where subscript $n$ denotes the time level, and $\Delta t = t_{n+1}-t_n$ is
the current time step.
The implementation goes like
!bc cod
class ForwardEuler(Solver):
    """
    Forward Euler scheme::

        u[n+1] = u[n] + dt*f(u[n], t[n])
    """
    quick_description = 'The simple explicit (forward) Euler scheme'

    def advance(self):
        u, f, n, t = self.u, self.f, self.n, self.t
        dt = t[n+1] - t[n]
        unew = u[n] + dt*f(u[n], t[n])
        return unew
!ec
Remarks:

 o The `quick_description` is necessary for the class to appear in the
   automatically generated overview of implemented methods
   (run `pydoc odespy` to see this table).
 o Extracting class attributes in local variables (here `u`, `f`, etc.)
   avoids the need for the `self` prefix so that the implemented formulas
   are as close to the mathematical formulas as possible.

Almost equally simple schemes, like explicit Runge-Kutta methods and Heun's
method are implemented in the same way (see `ODE.py`).

=== A Subclass with More Code ===

A 2nd-order Adams-Bashforth scheme is a bit more complicated since it
involves three time levels and therefore needs a separate method for
the first step. We should also avoid unnecessary evaluations of $f(u,t)$.
The user can specify a parameter `start_method` for the name of the
solver to be used for the first step. This solver is initialized
by the `switch_to` method in class `Solver`. Basically,
!bc cod
new_solver = solver.switch_to(solver_name)
!ec
creates a new solver instance `new_solver`, of the class implied by
`solver_name`, where all relevant parameters from `solver` are coopied
to `new_solver`.

An implementation of a subclass for the
2nd-order Adams-Bashforth scheme can then look as follows.
!bc cod
class AdamsBashforth2(Solver):
    """
    Second-order Adams-Bashforth method::

        u[n+1] = u[n] + dt/2.*(3*f(u[n], t[n]) - f(u[n-1], t[n-1]))

    for constant time step dt.

    RK2 is used as default solver in first step.
    """
    quick_description = "Explicit 2nd-order Adams-Bashforth method"

    _optional_parameters = Solver._optional_parameters + ['start_method',]

    def initialize_for_solve(self):
        # New solver instance for first step
        self.starter = self.switch_to(self.start_method)
        Solver.initialize_for_solve(self)

    def validate_data(self):
        if not self.constant_time_step():
            print '%s must have constant time step' % self.__name__
            return False
        else:
            return True

    def advance(self):
        u, f, n, t = self.u, self.f, self.n, self.t

        if n >= 1:
            dt = t[n+1] - t[n]  # must be constant
            self.f_n = f(u[n], t[n])
            unew = u[n] + dt/2.*(3*self.f_n - self.f_n_1)
            self.f_n_1 = self.f_n
        else:
            # User-specified method for the first step
            self.starter.set_initial_condition(u[n])
            time_points = [t[n], t[n+1]]
            u_starter, t_starter = self.starter.solve(time_points)
            unew = u_starter[-1]
            self.f_n_1 = f(u[0], t[0])

        return unew
!ec
Three features are worth comments: 1) we extend the set of optional
parameters; 2) we must initialize a separate solver for the first
step, and this is done in the `initialize_for_solve` method that will
be called as part of `solve` (before the time stepping); and 3) we
extend `validate_data` to check that the time spacing given by the
`time_points` argument to `solve` is constant. The utility method
`constant_time_step` provided in super class `Solver` carries out the
details of the check.

More advanced implementations of subclasses can be studied
in the `ODE.py` and `RungeKutta.py` files.

=== A Simple Example of an Implicit Method ===

Class `SolverImplicit` acts as superclass for the implementation of
implicit methods. This class provides some basic functionality for
solving the system of nonlinear equations that normally arise in
implicit methods by Picard or Newton iteration.
The parameter `nonlinear_solver` can take the values `Picard` or
`Newton`. The user must provide in case of Newton's method provide
a `jac` parameter for a function evaluating the Jacobian of $f(u,t)$
with respect to $u$: $J_{i,j} = \partial f_i/\partial u_j$.

Instead of implementing an `advance` method in subclasses, one provides
a method `Picard` and/or `Newton` to define key quantities in these
methods. The superclass implements `advance`, which will run a Picard
or Newton iteration. The `Picard` method returns all the terms
on the right-hand side of the discrete equation when only `u[n+1]` is
on the left-hand side. `Newton` returns the right-hand side and the
Jacobian of the system to be solved in each Newton iteration.

Here is an example showing how to implement the Backward Euler method.
!bc
class BackwardEuler(SolverImplicit):
    """
    Implicit Backward Euler method::

       u[n+1] = u[n] + dt*f(t[n+1], u[n+1])

    The nonlinear system is solved by Newton or Picard iteration.
    """
    quick_description = "Implicit 1st-order Backward Euler method"

    def Picard_update(self, ukp1):
        u, f, n, t = self.u, self.f, self.n, self.t
        dt = t[n+1] - t[n]
        return u[n] + dt*f(ukp1, t[n+1])

    def Newton_system(self, ukp1):
        u, f, n, t = self.u, self.f, self.n, self.t
        dt = t[n+1] - t[n]
        F = ukp1 - (u[n] + dt*f(ukp1, t[n+1]))
        J = np.eye(self.neq) - dt*self.jac(ukp1, t[n+1])
        return F, J
!ec

======= Installation =======

The `odespy` package is most easily installed by checkout out
the latest version of the source code:
!bc sys
git clone git@github.com:hplgit/odespy.git
cd odespy
!ec
The installation follows the expected standard procedure,
!bc sys
sudo python setup.py install
!ec

The `odespy` package depends on several other packages:

 * `scipy` for running the `Vode` Adams/BDF solver and the
   Dormand-Prince adaptive methods `Dop853`, and `Dopri5`.
 * `sympy` for running the extremely accurate `SymPy_odefun` solver.

These packages are readily downloaded and installed by the
standard `setup.py` script as shown above.
On Ubuntu and other Debian-based Linux systems the following
line installs all that `odespy` may need:
!bc sys
sudo apt-get install python-scipy python-nose python-sympy
!ec

# #ifdef EXTRA
======= TODO =======

 * Start tutorial with solving a real problem with vode and RK2 say;
   preferably some system where it is unclear if a sophisticated or
   standard method will work.
 * new parameter: casename, put in Solver, always have, but can be None
 * Incorporate odelab
 * Interface sundials
 * Show phase space plot
 * call scipy.optimize.fsolve for Newton?
 * Show how to store different simulations in file (shelve, Gael's
   work, NumPyDB), name each case, could have solve(..., storage='file')
   and then return u as some file-like object that accepts u[i] etc and
   for ui in u etc. Could be easy in class Solver. Requires param filename.
 * Design overview with superclasses
 * Storage (and philosophy)
 * SIR examples
 * Physics examples where class Problem has kinetic energy(u),
   potential energy(u), position, momentum, velocity, friction force,
   spring force, etc. Given u, get the physical quantity.
   Could be a general base class for Oscillators,
   odespy.Problem.Oscillator.
 * Complete physics example with class Problem, Solver and Viz a la INF1100.
   Adaptive error controlled solution.
 * Solvers for 2nd order eqs where f must be an object and also have a
   method d2ut2(u, dudt, t) for returning u''="f"?
 * PDE examples: conservation laws, diffusion
 * SODEs, need f as f + noise(u, t)? Problem: need dt, otherwise we
   could do this without any support. Solution: provide dt to
   f constructor :-)
 * Explain and show the code of the hierarchy.

# #endif
